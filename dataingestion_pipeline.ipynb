{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c867ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa46ca",
   "metadata": {},
   "source": [
    "SETUP & CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af16df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'source_file': 'dynamic_pricing.csv',\n",
    "    'raw_folder': 'data/raw',\n",
    "    'processed_folder': 'data/processed',\n",
    "    'logs_folder': 'logs',\n",
    "    'date_format': '%Y%m%d'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ad231",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c9bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(folders):\n",
    "    try:\n",
    "        logger.info(\"Creating project directories...\")\n",
    "        for folder in folders.values():\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            logger.info(f\" Created/verified: {folder}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create directories: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def validate_source_file(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        logger.error(f\"Source file not found: {filepath}\")\n",
    "        return False\n",
    "    if not filepath.endswith('.csv'):\n",
    "        logger.error(\"Invalid file format. Expected CSV file.\")\n",
    "        return False\n",
    "    logger.info(f\"Source file validated: {filepath}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_data(filepath):\n",
    "    try:\n",
    "        logger.info(f\"Extracting data from: {filepath}\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        logger.info(f\" Extracted {len(df)} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_schema(df):\n",
    "    required_columns = [\n",
    "        'Number_of_Riders',\n",
    "        'Number_of_Drivers',\n",
    "        'Location_Category',\n",
    "        'Customer_Loyalty_Status',\n",
    "        'Number_of_Past_Rides',\n",
    "        'Average_Ratings',\n",
    "        'Time_of_Booking',\n",
    "        'Vehicle_Type',\n",
    "        'Expected_Ride_Duration',\n",
    "        'Historical_Cost_of_Ride'\n",
    "    ]\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        logger.error(f\"Missing required columns: {missing}\")\n",
    "        return False, missing\n",
    "    logger.info(\" Schema validation passed\")\n",
    "    return True, []\n",
    "\n",
    "\n",
    "def check_data_quality(df):\n",
    "    logger.info(\"Checking data quality...\")\n",
    "    metrics = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'empty_dataset': len(df) == 0\n",
    "    }\n",
    "    if metrics['missing_values'] > 0:\n",
    "        logger.warning(f\"Found {metrics['missing_values']} missing values\")\n",
    "    if metrics['duplicate_rows'] > 0:\n",
    "        logger.warning(f\"Found {metrics['duplicate_rows']} duplicate rows\")\n",
    "    if metrics['empty_dataset']:\n",
    "        logger.error(\"Dataset is empty!\")\n",
    "    else:\n",
    "        logger.info(\" Data quality check completed\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_data(df, folder, filename_prefix):\n",
    "    try:\n",
    "        today = datetime.now().strftime(CONFIG['date_format'])\n",
    "        filepath = os.path.join(folder, f\"{filename_prefix}_{today}.csv\")\n",
    "        df.to_csv(filepath, index=False)\n",
    "        file_size = os.path.getsize(filepath) / 1024\n",
    "        logger.info(f\" Saved: {filepath} ({file_size:.1f} KB)\")\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_ingestion_report(metrics, files_created):\n",
    "    try:\n",
    "        today = datetime.now().strftime('%Y-%m-%d')\n",
    "        report_path = os.path.join(CONFIG['logs_folder'], f'ingestion_report_{today}.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            status = \"SUCCESS\" if not metrics['empty_dataset'] else \"FAILED\"\n",
    "            f.write(\"INGESTION STATUS\\n\")\n",
    "            f.write(f\"Status: {status}\\n\\n\")\n",
    "            f.write(\"DATA QUALITY METRICS\\n\")\n",
    "            f.write(f\"Total Records:      {metrics['total_rows']:,}\\n\")\n",
    "            f.write(f\"Total Features:     {metrics['total_columns']}\\n\")\n",
    "            f.write(f\"Missing Values:     {metrics['missing_values']}\\n\")\n",
    "            f.write(f\"Duplicate Records:  {metrics['duplicate_rows']}\\n\\n\")\n",
    "            f.write(\"FILES CREATED\\n\")\n",
    "            for idx, filepath in enumerate(files_created, 1):\n",
    "                f.write(f\"{idx}. {filepath}\\n\")\n",
    "        logger.info(f\"Report generated: {report_path}\")\n",
    "        return report_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate report: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311e9c5",
   "metadata": {},
   "source": [
    "INGESTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b9dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ingestion_pipeline():\n",
    "    print(\"EXECUTING DATA INGESTION PIPELINE\")\n",
    "    \n",
    "    print(\"Step 1: Creating project directories...\")\n",
    "    folders = {\n",
    "        'raw': CONFIG['raw_folder'],\n",
    "        'processed': CONFIG['processed_folder'],\n",
    "        'logs': CONFIG['logs_folder']\n",
    "    }\n",
    "    if not create_directories(folders):\n",
    "        print(\" Failed to create directories\")\n",
    "        return False\n",
    "    print(\"Directories ready\\n\")\n",
    "    \n",
    "    print(\"Step 2: Validating source file...\")\n",
    "    if not validate_source_file(CONFIG['source_file']):\n",
    "        print(\"Source file validation failed\")\n",
    "        return False\n",
    "    print(\"Source file validated\\n\")\n",
    "    \n",
    "    print(\"Step 3: Extracting data...\")\n",
    "    df = extract_data(CONFIG['source_file'])\n",
    "    if df is None:\n",
    "        print(\"Data extraction failed\")\n",
    "        return False\n",
    "    print(f\"Extracted {len(df):,} records\\n\")\n",
    "    \n",
    "    print(\"Step 4: Validating data schema...\")\n",
    "    is_valid, missing_cols = validate_schema(df)\n",
    "    if not is_valid:\n",
    "        print(f\"Schema validation failed. Missing: {missing_cols}\")\n",
    "        return False\n",
    "    print(\"Schema validated\\n\")\n",
    "    \n",
    "    print(\"Step 5: Checking data quality...\")\n",
    "    metrics = check_data_quality(df)\n",
    "    print(f\"Quality check completed\")\n",
    "    print(f\" Total rows: {metrics['total_rows']:,}\")\n",
    "    print(f\" Missing values: {metrics['missing_values']}\")\n",
    "    print(f\" Duplicates: {metrics['duplicate_rows']}\\n\")\n",
    "    \n",
    "    print(\"Step 6: Saving raw data backup...\")\n",
    "    raw_file = save_data(df, CONFIG['raw_folder'], 'pricing')\n",
    "    if raw_file is None:\n",
    "        print(\"Failed to save raw data\")\n",
    "        return False\n",
    "    print(f\"Raw data saved\\n\")\n",
    "    \n",
    "    print(\"Step 7: Saving processed data...\")\n",
    "    processed_file = save_data(df, CONFIG['processed_folder'], 'pricing')\n",
    "    if processed_file is None:\n",
    "        print(\"Failed to save processed data\")\n",
    "        return False\n",
    "    print(f\"Processed data saved\\n\")\n",
    "    \n",
    "    print(\"Step 8: Generating ingestion report...\")\n",
    "    files_created = [raw_file, processed_file]\n",
    "    report_file = generate_ingestion_report(metrics, files_created)\n",
    "    if report_file is None:\n",
    "        print(\"Failed to generate report\")\n",
    "        return False\n",
    "    print(f\"Report generated\\n\")\n",
    "    print(f\"\\n Files Created:\")\n",
    "    print(f\"  1. {raw_file}\")\n",
    "    print(f\"  2. {processed_file}\")\n",
    "    print(f\"  3. {report_file}\")    \n",
    "    logger.info(\"Pipeline execution completed successfully\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365c6db",
   "metadata": {},
   "source": [
    "EXECUTE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06ae221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 12:30:20 - INFO - Creating project directories...\n",
      "2025-10-07 12:30:20 - INFO -  Created/verified: data/raw\n",
      "2025-10-07 12:30:20 - INFO -  Created/verified: data/processed\n",
      "2025-10-07 12:30:20 - INFO -  Created/verified: logs\n",
      "2025-10-07 12:30:20 - INFO - Source file validated: dynamic_pricing.csv\n",
      "2025-10-07 12:30:20 - INFO - Extracting data from: dynamic_pricing.csv\n",
      "2025-10-07 12:30:20 - INFO -  Extracted 1000 rows, 10 columns\n",
      "2025-10-07 12:30:20 - INFO -  Schema validation passed\n",
      "2025-10-07 12:30:20 - INFO - Checking data quality...\n",
      "2025-10-07 12:30:20 - INFO -  Data quality check completed\n",
      "2025-10-07 12:30:20 - INFO -  Saved: data/raw\\pricing_20251007.csv (64.5 KB)\n",
      "2025-10-07 12:30:20 - INFO -  Saved: data/processed\\pricing_20251007.csv (64.5 KB)\n",
      "2025-10-07 12:30:20 - INFO - Report generated: logs\\ingestion_report_2025-10-07.txt\n",
      "2025-10-07 12:30:20 - INFO - Pipeline execution completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING DATA INGESTION PIPELINE\n",
      "Step 1: Creating project directories...\n",
      "Directories ready\n",
      "\n",
      "Step 2: Validating source file...\n",
      "Source file validated\n",
      "\n",
      "Step 3: Extracting data...\n",
      "Extracted 1,000 records\n",
      "\n",
      "Step 4: Validating data schema...\n",
      "Schema validated\n",
      "\n",
      "Step 5: Checking data quality...\n",
      "Quality check completed\n",
      " Total rows: 1,000\n",
      " Missing values: 0\n",
      " Duplicates: 0\n",
      "\n",
      "Step 6: Saving raw data backup...\n",
      "Raw data saved\n",
      "\n",
      "Step 7: Saving processed data...\n",
      "Processed data saved\n",
      "\n",
      "Step 8: Generating ingestion report...\n",
      "Report generated\n",
      "\n",
      "\n",
      " Files Created:\n",
      "  1. data/raw\\pricing_20251007.csv\n",
      "  2. data/processed\\pricing_20251007.csv\n",
      "  3. logs\\ingestion_report_2025-10-07.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        success = run_ingestion_pipeline()\n",
    "        if not success:\n",
    "            print(\"\\n Pipeline completed with errors. Check logs for details.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        print(f\"\\n Pipeline failed with error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
